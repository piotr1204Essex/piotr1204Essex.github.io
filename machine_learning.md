---
layout: machine_learning
title: Machine Learning
---

### Collaborative Discussion 1: The 4th Industrial Revolution

#### Initial Post

The topics discussed in Schwab's (2017) book on the fourth industrial revolution concern a sector I am involved in professionally - that is Big Data. Machine Learning and Artificial Intelligence cannot be created or work or exist or provide any value without the data that fuels them. It is fair to say that the significant advancements in ML and AI fields are made available thanks to the exponential increase in the data we collect globally, as humanity (Anon, 2022).

The availability of big data enables organisations to operate and scale more efficiently. With the increased reliance on data there is a growing risk of an incident with serious repercussions, should the data become unavailable, for whatever reason.

This brings me to a specific incident where information systems failed: the 2017 WannaCry ransomware attack (Julia Carrie Wong and Solon, 2017). This attack focused on encrypting organisations’ data and demanding a ransom in order for them to regain access. For private businesses this had widespread consequences that affected business continuity and resulted in monetary losses. Unfortunately also government organisations have been a target, with the UK’s NHS patients suffering, arguably, the most having thousands of their appointments cancelled and others having experienced disrupted patient care. This shows how important and crucial asset data is. By restricting access to it, malicious actors have caused pain and suffering - for NHS’s patients, monetary loss for businesses, disrupted usage of products of many companies (arguably the lowest level impact) by their customers and caused a significant reputational cost to all of the organisations affected. The failure of the systems that were hacked lays within the lack of proper cyber security measures that should have prevented attackers from gaining access to the data in the first place.

This example clearly proves that with great power, that big data gives us, huge responsibility comes. I like to think about it in a similar way one could think about household finances. The lower the number of streams of income, the higher reliance factor is associated with every single source of income, the higher risk of defaulting on household payments. If we, as humanity, continuously increase reliance on data and information systems - we must be prepared for an increased risk of “defaulting” on the delivery promised by these information systems.

References:

1. Schwab, K. (2017) The Fourth Industrial Revolution. London: Penguin Random House. 
2. Anon, (2022). Big Data And Artificial Intelligence: What’s The Future For Them? [online] Available at: https://dataconomy.com/2022/11/07/big-data-and-artificial-intelligence/?utm_content=cmp-true [Accessed 3 May 2023].
‌3. Julia Carrie Wong and Solon, O. (2017). Massive ransomware cyber-attack hits nearly 100 countries around the world. [online] the Guardian. Available at: https://www.theguardian.com/technology/2017/may/12/global-cyber-attack-ransomware-nsa-uk-nhs [Accessed 3 May 2023].

#### Summary Post

The discussion in this thread is focused on the significance of BigData in the 4th Industrial Revolution as well as on potential repercussions and security risks related to increased reliance on technology. It is clear to me how BigData is fueling advancements in Artificial Intelligence as well as in Machine Learning fields, and my colleagues seem to agree. With BigData come big responsibilities and only greater risks. The WannaCry ransomware attack being one of the examples where malicious actors caused a significant disruption to many businesses, but also for the UK’s National Health Service - which was particularly painful for the UK’s population. Many patients had their appointments cancelled or rescheduled as a result of these actions. One can only imagine what hardship must they have gone through, especially if waiting for a life-or-death kind of surgery. 

One of the points my initial post was challenged on is the comparison to household finances - when it comes to reliance on a single source of income (or truth - in case of data). It was found to be thought-provoking but required more context and explanation - which was supported in the reply. It was valuable learning for me, as this kind of comparison was self-explanatory to me, but I learned how important it is to make sure my communication is clear, as not everyone can work based on similar assumptions to myself. 

The summary of this discussion for me is the following: with BigData comes big responsibility. We are staying on the verge of a historical breakthrough that will increase the standard of living for so many people. But this opportunity comes at a hidden cost that we all must take into account. Putting protective measures in place, ensuring security of BigData, AI and ML should be at the top of the list of priorities. 

### Collaborative Discussion 2: Legal and Ethical views on ANN applications

#### Initial Post

Hutson (2021) discusses the capabilities of AI potentially serving a function of an actual writer - hence ‘Robo-writers’. They are built on sophisticated AI models that can swiftly generate text on a variety of topics. Their application, however, is not devoid of cons.

The advantages of employing AI writing tools seem self-explanatory. They can improve efficiency in daily tasks, particularly in generating mass communication like emails and reports swiftly and with precision (Hutson, 2021). In terms of creativity, these AI platforms can act as thought catalysts, helping writers overcome creative hurdles (Grosz, 2022). For instance, GPT-3's ability to create poetry and short narratives illustrates the extent of AI's creative assistance (Clark et al., 2020).

Simultaneously, the use of AI in writing raises several challenging issues. Ethical considerations are paramount, given concerns about the originality and authenticity of AI-produced content (Boddington, 2021). AI tools work by identifying and following patterns in data, leading to the possibility of unintended replication of existing content and potential copyright issues (Boddington, 2021). We must also remember that it is this very source data that feeds creations of AI. Human soul is something many cannot fathom and many would argue machine can never attain.

In summary, AI writing tools bring considerable benefits, including productivity enhancement and creative stimulation. Yet, they also come with significant drawbacks, including ethical dilemmas, societal implications related to job loss, and bias amplification. To maximise AI's benefits in writing, these issues need to be addressed, perhaps through ethical guidelines, employment protection measures, and bias reduction strategies.

References:

1. Boddington, P. (2021). Ethical guidelines for AI: A proposed framework. Springer.

2. Clark, E., et al. (2020). "ELECTRA: Rethinking pre-training text encoders." arXiv preprint arXiv:2003.10555.

3. Grosz, B. (2022). AI's impact on the future job landscape. AI Magazine.

4. Hutson, M. (2021). The pros and cons of AI-based writing tools. Nature.

#### Summary Post

The discussion here revolves around Robo-writers as discussed by Hutson (2021) in Nature. It is becoming increasingly clear how capable and powerful generative AI is. It also shows where it can get with the right guidance and nurture. 

It is highlighted that while we may observe numerous benefits to the usage of Robo-writers, their application is not devoid of cons. The AI might support administrative, repeatable tasks that are based on a pre-set of data or curated input. It does seem to do a very good job at it. Some of the applications one can think of, are mostly within public or corporate space. Drafting mass communication emails, instructions for the products, tutorials for manual set up at home and so on and so forth. All of what has been mentioned though, is quite far away from creative literature of stories, heroes, fairy tales etc.

Although the AI seems to be handling creativity just fine (Grosz, 2022), there is the concept of a human soul. Something spiritual, unique to each and everyone of us. Something that animals do not possess. If that is the case, then how could an AI have a soul? And without a soul, how can a novel captivate the reader? There is no academic resource I could find to back the above statement up. Soul is a relative term and is open to various interpretations. It is however noteworthy when considering Robo-writers.

The summary is similar to the one from the initial post. So I will leave the discussion here, with the food for thought in the form of Robo-writers versus human soul.

References:

1. Boddington, P. (2021). Ethical guidelines for AI: A proposed framework. Springer.

2. Clark, E., et al. (2020). "ELECTRA: Rethinking pre-training text encoders." arXiv preprint arXiv:2003.10555.

3. Grosz, B. (2022). AI's impact on the future job landscape. AI Magazine.

4. Hutson, M. (2021). The pros and cons of AI-based writing tools. Nature.

### e-Portfolio Activity: Correlation and Regression

#### Unit 3

When I started to look at the notebooks provided and I found my way around them, I realised very quickly that every change in data points can have an impact on correlation or regression. What caught my attention initially, was modifying the multiplier for the first data set, and as I was increasing the number, the correlation was flattening more and more. See below example:

![Alt text for image](/assets/images/c1.png)

<div align="center">
<p> Figure 1: Initial Graph. </p>
</div>

![Alt text for image](/assets/images/c2.png)

<div align="center">
<p> Figure 2: Graph where first variable had increased multiplier to 100. </p>
</div>

![Alt text for image](/assets/images/c3.png)

<div align="center">
<p> Figure 3: Graph where first variable had increased multiplier to 1000. </p>
</div>

Further playing with data only brought more fun and interesting learnings. It has also prompted me to research a bit more on the topic.

As data is integral to statistical analysis and both correlation and regression are statistical ways of measurement - it only comes natural, that any change in them can have a profound impact on any analysis.

What I found to be detrimental to correlation is outliers. Because they deviate from the rest of the data significantly, they can also significantly impact the output of a correlation calculation (Ravichandran, 2009). Outlier, as the name suggests, is something that is out of order and does not fall into the usual behaviour of the data points. Which is why there are statistical best practices on how to identify and then exclude such points in order to run the correlation exercise correctly (Abhigyan, 2020).

When it comes to regression, I believe there is no need to repeat myself, as the effects are very similar to those on correlation. Outliers can seriously affect the regression line which in turn leads to incorrect interpretations (James et al., 2013). 

I have always believed that Data Science is a science of a “best-guess”. It is hard to imagine being 100% confident in any statistical analysis that one runs. It is because of that, that we must adhere to the best practices and all of the tools that statistics provides us with, so we can make sure that our best-guess is indeed the best-guess one can make. 

References:

1. Abhigyan (2020) Detecting and removing outliers., Medium. Available at: https://medium.com/analytics-vidhya/detecting-and-removing-outliers-7b408b279c9 (Accessed: 12 June 2023). 

2. Ravichandran, K. S. (2009). A First Course in Linear Model Theory. CRC Press.

3. James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning: with Applications in R. Springer.

#### Unit 4

Unit 4's tasks have also revolved around correlation and regression. This time we were looking at the world's population and GDP. Disclaimer: the datasets provided had the data only up until (and including) year 2020, hence all results reflect this material change accordingly.

![Alt text for image](/assets/images/unit_4_task_a.png)

<div align="center">
<p> Figure 1: Correlation between Mean Population and GDP (2001-2020).</p>
<p> Pearsons correlation coefficient: 0.031 </p>
</div>

Looking at Figure 1 we can clearly identify several outliers on both axes. The vast majority of the data however, sits within 0-3 (1e13) the mean per capita GDP and 0-3 (1e9) the mean population. What's caught my attention is that the data points seem to "stick" to the axes lines. Which means - either they grow on the mean per capita GDP - but very little on the mean population or they grow on the mean population but very little on the mean per capita GDP. This kind of reading would represent two main types of countries' behaviours. First one would be a natural growth of the country with retained mean per capita GDP value - something one could classify as stable and sustainable growth at a pace that allows each new member of the society to uphold the standard of living of the other ones. It also means no economic of financial status advancement of a country's population. The second one being high increase in financial status of country's citizens, but little to no growth in the mean population.

The Pearsons correlation coefficient result suggests that, in general, as the mean population of a country increases, the mean per capita GDP for that country also slightly increases. However, the relationship is very weak, meaning there's a lot of scatter and the data points don't fall close to a straight line.
Therefore, it would be inappropriate to make strong predictions about a country's mean per capita GDP from 2001 to 2021 based solely on its mean population from the same period. There are likely other factors at play that affect a country's mean per capita GDP which are not accounted for by the population alone.

Second task from this Unit was to create a regression where the independent variable is the mean population of each country (from 2001 to 2021) and dependent variable is mean per capita GDP (from 2001 to 2021). See below the graph visualising Linear Regression model results.

![Alt text for image](/assets/images/unit_4_task_b.png)

<div align="center">
<p> Figure 2: Regression line </p>
</div>
- Root Mean Squared Error: ~8884886434480.2
- R-squared: ~ -0.011

There is no new revelation that comes with this LR model. The first task has already shown strongly the relationship between the variables (or mostly lack thereof). I have nothing more to add than the summary from the previous points. Please find below to full source code for these tasks.

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

gdp_df = pd.read_csv('Unit04 Global_GDP.csv')
pop_df = pd.read_csv('Unit04 Global_Population.csv')

# Using ffill to fill the missing values
gdp_df.fillna(method="ffill", inplace=True)
pop_df.fillna(method="ffill", inplace=True)

pop_df.loc[:, '2001':'2020'] = pop_df.loc[:, '2001':'2020'].apply(pd.to_numeric, errors='coerce')

mean_gdp = gdp_df.loc[:, '2001':'2020'].mean(axis=1)
mean_pop = pop_df.loc[:, '2001':'2020'].mean(axis=1)

df = pd.DataFrame({'Country': gdp_df['Country Name'], 'Mean_GDP': mean_gdp, 'Mean_Population': mean_pop})

plt.figure(figsize=(10,6))
plt.scatter(df['Mean_Population'], df['Mean_GDP'])
plt.xlabel('Mean Population (2001-2020)')
plt.ylabel('Mean GDP (2001-2020)')
plt.title('Correlation between Mean Population and GDP (2001-2020)')
plt.show()

df.fillna(method="ffill", inplace=True)

corr, _ = pearsonr(df['Mean_Population'], df['Mean_GDP'])
print('Pearsons correlation coefficient: %.3f' % corr)

# Linear Regression model

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn import metrics

# Creating X and y
X = df['Mean_Population'].values.reshape(-1,1)
y = df['Mean_GDP'].values.reshape(-1,1)

# Splitting the data into training and testing data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Creating and training model
model = LinearRegression()  
model.fit(X_train, y_train)

# Model prediction on train data
y_pred = model.predict(X_test)

# Model Evaluation
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))
print('R-squared:', metrics.r2_score(y_test, y_pred))

# Plotting the regression line
plt.scatter(X_test, y_test,  color='gray')
plt.plot(X_test, y_pred, color='red', linewidth=2)
plt.show()
```
#### Unit 5

During Unit 5 we were presented with a table (see below) showing pathological test results for three individuals.

![Alt text for image](/assets/images/Screenshot 2023-07-18 at 17.42.39.png)

<div align="center">
<p> Figure 1: Table with test results </p>
</div>

We were asked to calculate Jaccard coefficient for the following pairs:

- (Jack, Mary)
- (Jack, Jim)
- (Jim, Mary)

We will assume that sets per person are containing Fever as well as any other column with a value of "P". Then we will proceed to calculating coefficients.

Jack: {Fever, Test-1}
Mary: {Fever, Test-1, Test-3}
Jim: {Fever, Cough}

Now, let's calculate the Jaccard coefficient for each pair:

1. (Jack, Mary)

Intersection = {Fever, Test-1} = 2 elements
Union = {Fever, Test-1, Test-3} = 3 elements
Jaccard coefficient = Intersection / Union = 2/3 = 0.67 (approx)

2. (Jack, Jim)

Intersection = {Fever} = 1 element
Union = {Fever, Cough, Test-1} = 3 elements
Jaccard coefficient = Intersection / Union = 1/3 = 0.33 (approx)

3. (Jim, Mary)

Intersection = {Fever} = 1 element
Union = {Fever, Cough, Test-1, Test-3} = 4 elements
Jaccard coefficient = Intersection / Union = 1/4 = 0.25

There is no further instructions in the task as to how to close it, so I will simply summarise, that the highest Jaccard coefficient score has the Jack & Mary pair.

### Development Team Project: Project Report

At the end of Unit 6 we were due to submit the Development Team Project. I think the task was a very interesting one. We were asked to take a dataset containing listing's information from Airbnb publicly provided data, and try to answer an interesting question, that could be useful for the Airbnb's management team. We have asked ourselves: "Which feature most strongly affects the Airbnb listing
price?"

I, personally, have always found it fascinating, how prices for different services and products are developed. Is it the location, quality, size or maybe only inflation rate, that affects the change/amplitude in the prices across different services/products?

Working together with Rory was a huge learning experience for me. I found it extremely beneficial, that both of us have approached this task from different angles. In turn we could learn from each other as well as merge the results of our work into a coherent piece. 

Please find below the code we have used to answer the above set question:

```python
# %% [markdown]
# # Airbnb Analysis

# %% [markdown]
# ## Data loading and preparation

# %%
import numpy as np
import pandas as pd
import scipy.stats as st
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import seaborn as sns
import missingno as msno
import warnings
warnings.filterwarnings('ignore')

# %%
airbnb=pd.read_csv('AB_NYC_2019.csv')

airbnb.fillna({'reviews_per_month':0,
              'name':'NA'}, inplace=True)

airbnb = airbnb[airbnb.price>0]

# %% [markdown]
# ### Exploratory Data Analysis: summary statistics including distribution

# %%
airbnb.describe()

# %%
airbnb.head()

# %%
airbnb.tail()

# %%
airbnb.shape

# %%
numeric_features = airbnb.select_dtypes(include=[np.number])
numeric_features.columns

# %%
categorical_features = airbnb.select_dtypes(exclude=[np.number])
categorical_features.columns

# %%
msno.matrix(airbnb.sample(250))

# %%
msno.dendrogram(airbnb)

# %%
sns.heatmap(airbnb.corr(method='kendall',
                  numeric_only=True), cmap='coolwarm')
plt.show()

# %% [markdown]
# ### Distribution of target (price)

# %%
sns.histplot(data=airbnb, x='price', binrange=(0,1000), bins=30)

# %%
airbnb['logprice'] = np.log2(airbnb.price)

# %%
sns.histplot(data=airbnb, x='logprice', bins=30)

# %% [markdown]
# ## Data processing
# 
# ### One hot encode categorical features

# %%
y = airbnb['logprice']

x = airbnb[['neighbourhood_group',
                   'latitude',
                   'longitude',
                   'room_type',
                   'minimum_nights',
                   'number_of_reviews',
                   'calculated_host_listings_count',
                   'availability_365']]

x_oh = pd.get_dummies(x, columns=['room_type','neighbourhood_group'])

# %% [markdown]
# ## Models
# 
# ### Imports for models

# %%
import sklearn
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.inspection import permutation_importance

# %% [markdown]
# ### 1. Linear regression

# %%
x_train,x_test,y_train,y_test=train_test_split(x_oh,y,test_size=.1,random_state=353)

model = LinearRegression()

model.fit(x_train,y_train)

# %%
y_pred=model.predict(x_train)
r2_score(y_train,y_pred)

# %%
y_pred=model.predict(x_test)
r2_score(y_test,y_pred)

# %%
importance = permutation_importance(model, x_test, y_test, scoring='neg_root_mean_squared_error').importances_mean

fig=sns.barplot(x=x_train.columns.tolist(), y=importance)
fig.set_title('Permutation importance of features in linear model')
fig.set_xticklabels(fig.get_xticklabels(), rotation=90)
plt.savefig('plots/perm_imp_linear.pdf', bbox_inches='tight')

# %% [markdown]
# ### 2. Decision tree regression

# %%
from sklearn.tree import DecisionTreeRegressor

model = DecisionTreeRegressor()

model.fit(x_train, y_train)

importance = model.feature_importances_

# %%
y_pred=model.predict(x_train)

r2_score(y_train,y_pred)

# %%
y_pred=model.predict(x_test)

r2_score(y_test,y_pred)

# %%
fig=sns.barplot(x=x_train.columns.tolist(), y=importance)
fig.set_title('GINI importance of features in decisiontree model')
fig.set_xticklabels(fig.get_xticklabels(), rotation=90)
plt.savefig('plots/gini_imp_dtree.pdf', bbox_inches='tight')

# %% [markdown]
# ### 3. Random forest regression

# %%
from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor()

model.fit(x_train, y_train)

importance = model.feature_importances_

# %%
y_pred=model.predict(x_train)

r2_score(y_train,y_pred)

# %%
y_pred=model.predict(x_test)

r2_score(y_test,y_pred)

# %%
fig=sns.barplot(x=x_train.columns.tolist(), y=importance)
fig.set_title('GINI importance of features in RF model')
fig.set_xticklabels(fig.get_xticklabels(), rotation=90)
plt.savefig('plots/gini_imp_randforest.pdf', bbox_inches='tight')

# %%
importance = permutation_importance(model, x_test, y_test, scoring='neg_root_mean_squared_error').importances_mean

fig=sns.barplot(x=x_train.columns.tolist(), y=importance)
fig.set_title('Permutation importance of features in RF model (test)')
fig.set_xticklabels(fig.get_xticklabels(), rotation=90)
plt.savefig('plots/perm_imp_test_randforest.pdf', bbox_inches='tight')

# %%
importance = permutation_importance(model, x_train, y_train, scoring='neg_root_mean_squared_error').importances_mean

fig=sns.barplot(x=x_train.columns.tolist(), y=importance)
fig.set_title('Permutation importance of features in RF model (train)')
fig.set_xticklabels(fig.get_xticklabels(), rotation=90)
plt.savefig('plots/perm_imp_train_randforest.pdf', bbox_inches='tight')
plt.show()

# %% [markdown]
# ### 4. XGBoost regression

# %%
from xgboost import XGBRegressor

model = XGBRegressor()

model.fit(x_train, y_train)

importance = model.feature_importances_

# %%
y_pred=model.predict(x_train)

r2_score(y_train,y_pred)

# %%
y_pred=model.predict(x_test)

r2_score(y_test,y_pred)

# %%
fig=sns.barplot(x=x_train.columns.tolist(), y=importance)
fig.set_title('Inbuilt importance of features in XGBoost model (train)')
fig.set_xticklabels(fig.get_xticklabels(), rotation=90)
plt.savefig('plots/gini_imp_xgboost.pdf', bbox_inches='tight')

# %%
importance = permutation_importance(model, x_train, y_train, scoring='neg_root_mean_squared_error').importances_mean

fig=sns.barplot(x=x_train.columns.tolist(), y=importance)
fig.set_title('Permutation importance of features in XGBoost model (train)')
fig.set_xticklabels(fig.get_xticklabels(), rotation=90)
plt.savefig('plots/perm_imp_xgboost_train.pdf', bbox_inches='tight')

# %%
importance = permutation_importance(model, x_test, y_test, scoring='neg_root_mean_squared_error').importances_mean

# %%
fig=sns.barplot(x=x_train.columns.tolist(), y=importance)
fig.set_title('Permutation importance of features in XGBoost model (test)')
fig.set_xticklabels(fig.get_xticklabels(), rotation=90)
plt.savefig('plots/perm_imp_xgboost_test.pdf', bbox_inches='tight')

# %% [markdown]
# ## SHapley Additive exPlanations (SHAP)

# %%
import shap
shap.initjs()

x_train,x_test,y_train,y_test=train_test_split(x_oh,y,test_size=.1,random_state=353)

x_train_summary = shap.kmeans(x_train, 10)

# %% [markdown]
# ### 1. Linear regression SHAP

# %%
model = LinearRegression()

model.fit(x_train,y_train)

ex = shap.KernelExplainer(model.predict, x_train_summary)

shap_values = ex.shap_values(x_test[0:100])

shap.summary_plot(shap_values, x_test[0:100],show=False)
plt.savefig('plots/shap_linear_summary.pdf')

# %% [markdown]
# ### 2. Decision tree regression SHAP

# %%
from sklearn.tree import DecisionTreeRegressor

model = DecisionTreeRegressor()

model.fit(x_train, y_train)
ex = shap.TreeExplainer(model)
shap_values = ex.shap_values(x_test[0:200])
shap.summary_plot(shap_values, x_test[0:200],show=False)
plt.savefig('plots/shap_dtree_summary.pdf')

# %% [markdown]
# ### 3. XGBoost regression SHAP

# %%
from xgboost import XGBRegressor

model = XGBRegressor()

model.fit(x_train, y_train)

ex = shap.TreeExplainer(model)

shap_values = ex.shap_values(x_test[0:200])

shap.summary_plot(shap_values, x_test[0:200], show=False)
plt.savefig('plots/shap_xgboost_summary.pdf')
```
I personally contributed to both the EDA as well as choosing the XGBoost algorithm for solving this particular task. It was the first time for me to apply this specific algorithm. I learned a lot. More of the reflection on this project is available as the part of this module's personal reflection available [here](https://github.com/piotr1204Essex/piotr1204Essex.github.io/blob/main/ml_source/P.%20Sieminski%20Machine%20Learning%20May%202023%20Individual%20Reflection%20(1).pdf)

References: 

1. Chen, T. and Guestrin, C., 2016. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 785-794). ACM.

2. Friedman, J.H., 2001. Greedy function approximation: a gradient boosting machine. Annals of Statistics, pp.1189-1232.

### KMeans clustering (Unit 6)

#### Task A

I have been given a task to perform K-means clustering on a provided set of data ([which you can find here](https://github.com/piotr1204Essex/piotr1204Essex.github.io/blob/main/ml_source/Unit06%20iris.csv))

You can find below the source code for solving this task, as well as two visualisations that help understand the output.

```python
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, classification_report

df = pd.read_csv('Unit06 iris.csv')

le = LabelEncoder()
df['species'] = le.fit_transform(df['species'])

X = df.drop('species', axis=1)
y = df['species']

kmeans = KMeans(n_clusters=3, random_state=0).fit(X)

print(classification_report(y, kmeans.labels_))
print(confusion_matrix(y, kmeans.labels_))
```

![Alt text for image](/assets/images/task_a_1.png)

<div align="center">
<p> Scatter plot </p>
</div>


![Alt text for image](/assets/images/task_a_2.png)

<div align="center">
<p> Pair plot </p>
</div>

References:

1. Iris (no date) UCI Machine Learning Repository. Available at: https://archive.ics.uci.edu/dataset/53/iris (Accessed: 14 July 2023). 

#### Task B

I have been given a task to perform K-means clustering on a provided set of data ([which you can find here](https://github.com/piotr1204Essex/piotr1204Essex.github.io/blob/main/ml_source/Unit06%20wine.csv))

I have dropped the "Wine" column in order to select features for clustering. The clustering has been achieved with the following code:

```python
import pandas as pd
from sklearn.cluster import KMeans
data = pd.read_csv("Unit06 wine.csv")
X = data.drop('Wine', axis=1)

kmeans = KMeans(n_clusters=3, random_state=0).fit(X)
```

And I have chosen a slightly different approach to visualise the output - in order to show the different possibilities across KMeans clustering outputs. Please find below the output visualisation for this algorithm.

![Alt text for image](/assets/images/task_b_1.png)

<div align="center">
<p> Scatter plot with labels and cluster centers </p>
</div>

References:

1. Wine (no date) UCI Machine Learning Repository. Available at: https://archive.ics.uci.edu/dataset/109/wine (Accessed: 14 July 2023). 

#### Task C

I have been given a task to perform K-means clustering on a provided set of data ([which you can find here](https://github.com/piotr1204Essex/piotr1204Essex.github.io/blob/main/ml_source/Unit06%20weatherAUS.csv))

Columns 'RainToday' and 'RainTomorrow' contain categorical values, 
so I had to transform them into numerical values first. Then I filled the missing values with forward fill and by that time we were ready to perform KMeans clustering. As the task assumed testing KMeans for different values of K (from K=2 to K=6), I have written a loop that performs the clustering and produces a scatter plot as a visualisation output of the algorithm that allows to asses its output.

Please find below the Python source code:

```python
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt

df = pd.read_csv('Unit06 weatherAUS.csv')

le = LabelEncoder()
df['RainToday'] = le.fit_transform(df['RainToday'])
df['RainTomorrow'] = le.fit_transform(df['RainTomorrow'])

df.fillna(method ='ffill', inplace = True)

for k in range(2, 7):
    kmeans = KMeans(n_clusters=k)
    kmeans.fit(df)
    y_kmeans = kmeans.predict(df)
    plt.figure(figsize=(6, 6))
    plt.scatter(df.iloc[:, 0], df.iloc[:, 1], c=y_kmeans, s=50, cmap='viridis')
    centers = kmeans.cluster_centers_
    plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);
    plt.title('KMeans with k=' + str(k))
    plt.show()
```

Below you can find output scatter plots for each of the configurations.

![Alt text for image](/assets/images/task_c_1.png)

<div align="center">
<p>Scatter plot for K=2</p>
</div>

![Alt text for image](/assets/images/task_c_2.png)

<div align="center">
<p> Scatter plot for K=3 </p>
</div>

![Alt text for image](/assets/images/task_c_3.png)

<div align="center">
<p> Scatter plot for K=4 </p>
</div>

![Alt text for image](/assets/images/task_c_4.png)

<div align="center">
<p> Scatter plot for K=5 </p>
</div>

![Alt text for image](/assets/images/task_c_5.png)

<div align="center">
<p> Scatter plot for K=6 </p>
</div>

Assesing the outputs it does seem that K=3 is the most optimal output, which is not over-fitted, without too much noise, and is able to cluster the data quite neatly.

References:

1. Kerneler (2019) Starter: Weatheraus 536C1115-4, Kaggle. Available at: https://www.kaggle.com/code/kerneler/starter-weatheraus-536c1115-4/notebook (Accessed: 14 July 2023). 

### Unit 7
### Unit 8
### Unit 9
### Unit 10
### ROC & R2 metrics

In Unit 11 the activity we have been asked to perform was about changing parameters in a provided Jupyter Notebook in order to observe the changes in AUC and R2 metrics. I had some previous professional experience with regression, AUC and R2 metrics so I will delve into what my experience with this (and other) notebooks was. 

AUC (Area Under the Curve) is a metric used to assess the model’s capabilities in distinguishing between different classes. The larger AUC is, the better the model’s performance. What I found to have the biggest impact on AUC was the decision threshold. I have played as well with regularisation (logistic regression) and depth parameters (decision tree) that can easily lead to underfitting or overfitting - if set incorrectly (Fawcett, 2006). 

R2 is a metric that helps to understand the variance in the dependent variable and is used for regression problems. I like to think about the R2 score as a value that tells me how well it is a given model explaining the variance. Outliers can be some of the factors that decrease the R2 score and hence should be handled appropriately. It is also the choice of a degree of a polynomial regression (if applicable) that can strongly affect R2 value (James et al., 2013). 

References:

1. Fawcett, T. (2006). An introduction to ROC analysis. Pattern recognition letters, 27(8), 861-874.
2. James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning: with applications in R. Springer.

### Summative Assessment: Team Presentation

Unit 11 brought to us also a deadline for the second Team project we were asked to submit. This task required from us to develop a Neural Networks Algorithm based on a CIFAR-10 dataset. 
I have learned a lot during my own research, working with my team mate and during seminar with my tutor. Once again, me and my team mate have found ourselves approaching this task from different angles, which allowed us not only to learn from one another but also to create a coherent presentation as an output, that I believe fulfills the requirements of the task to its full extension. 

My personal contribution focused on optimising a CNN model for performance on the CIFAR-10 dataset. I started with an ANN model initially, but quickly realised it might be tought to get high performance out of this basic Neural Network structure - which is why I switched to a Convolutional Neural Network algorithm. I have taken a considerable amount of time to research various optimisation techniques and general structures of the CNN which allowed me to create a final output, which together with my team mate's contribution was a part of our final submission. More of the reflection on this project is available as the part of this module's personal reflection available [here](https://github.com/piotr1204Essex/piotr1204Essex.github.io/blob/main/ml_source/P.%20Sieminski%20Machine%20Learning%20May%202023%20Individual%20Reflection%20(1).pdf)

Please find below the links to the submission components (full presentation is not included as file is too large, but slides, source code and transcript is provided below):

- [Slides](https://github.com/piotr1204Essex/piotr1204Essex.github.io/blob/main/ml_source/Slides_%20Rory%20%26%20Piotr%20Neural%20Network%20Model.pdf)
- [Source code](https://github.com/piotr1204Essex/piotr1204Essex.github.io/blob/main/ml_source/source_code_piotr_rory_ml.py)
- [Transcript](https://github.com/piotr1204Essex/piotr1204Essex.github.io/blob/main/ml_source/transcript_piotr_rory_cifar_10_ml.pdf)

Both slide deck and transcript contain all relevant references.

### Unit 12
