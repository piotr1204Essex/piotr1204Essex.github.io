---
layout: machine_learning
title: Machine Learning
---

### Collaborative Discussion 1: The 4th Industrial Revolution

#### Initial Post

The topics discussed in Schwab's (2017) book on the fourth industrial revolution concern a sector I am involved in professionally - that is Big Data. Machine Learning and Artificial Intelligence cannot be created or work or exist or provide any value without the data that fuels them. It is fair to say that the significant advancements in ML and AI fields are made available thanks to the exponential increase in the data we collect globally, as humanity (Anon, 2022).

The availability of big data enables organisations to operate and scale more efficiently. With the increased reliance on data there is a growing risk of an incident with serious repercussions, should the data become unavailable, for whatever reason.

This brings me to a specific incident where information systems failed: the 2017 WannaCry ransomware attack (Julia Carrie Wong and Solon, 2017). This attack focused on encrypting organisations’ data and demanding a ransom in order for them to regain access. For private businesses this had widespread consequences that affected business continuity and resulted in monetary losses. Unfortunately also government organisations have been a target, with the UK’s NHS patients suffering, arguably, the most having thousands of their appointments cancelled and others having experienced disrupted patient care. This shows how important and crucial asset data is. By restricting access to it, malicious actors have caused pain and suffering - for NHS’s patients, monetary loss for businesses, disrupted usage of products of many companies (arguably the lowest level impact) by their customers and caused a significant reputational cost to all of the organisations affected. The failure of the systems that were hacked lays within the lack of proper cyber security measures that should have prevented attackers from gaining access to the data in the first place.

This example clearly proves that with great power, that big data gives us, huge responsibility comes. I like to think about it in a similar way one could think about household finances. The lower the number of streams of income, the higher reliance factor is associated with every single source of income, the higher risk of defaulting on household payments. If we, as humanity, continuously increase reliance on data and information systems - we must be prepared for an increased risk of “defaulting” on the delivery promised by these information systems.

References:

1. Schwab, K. (2017) The Fourth Industrial Revolution. London: Penguin Random House. 
2. Anon, (2022). Big Data And Artificial Intelligence: What’s The Future For Them? [online] Available at: https://dataconomy.com/2022/11/07/big-data-and-artificial-intelligence/?utm_content=cmp-true [Accessed 3 May 2023].
‌3. Julia Carrie Wong and Solon, O. (2017). Massive ransomware cyber-attack hits nearly 100 countries around the world. [online] the Guardian. Available at: https://www.theguardian.com/technology/2017/may/12/global-cyber-attack-ransomware-nsa-uk-nhs [Accessed 3 May 2023].

#### Summary Post

The discussion in this thread is focused on the significance of BigData in the 4th Industrial Revolution as well as on potential repercussions and security risks related to increased reliance on technology. It is clear to me how BigData is fueling advancements in Artificial Intelligence as well as in Machine Learning fields, and my colleagues seem to agree. With BigData come big responsibilities and only greater risks. The WannaCry ransomware attack being one of the examples where malicious actors caused a significant disruption to many businesses, but also for the UK’s National Health Service - which was particularly painful for the UK’s population. Many patients had their appointments cancelled or rescheduled as a result of these actions. One can only imagine what hardship must they have gone through, especially if waiting for a life-or-death kind of surgery. 

One of the points my initial post was challenged on is the comparison to household finances - when it comes to reliance on a single source of income (or truth - in case of data). It was found to be thought-provoking but required more context and explanation - which was supported in the reply. It was valuable learning for me, as this kind of comparison was self-explanatory to me, but I learned how important it is to make sure my communication is clear, as not everyone can work based on similar assumptions to myself. 

The summary of this discussion for me is the following: with BigData comes big responsibility. We are staying on the verge of a historical breakthrough that will increase the standard of living for so many people. But this opportunity comes at a hidden cost that we all must take into account. Putting protective measures in place, ensuring security of BigData, AI and ML should be at the top of the list of priorities. 

### e-Portfolio Activity: Correlation and Regression

When I started to look at the notebooks provided and I found my way around them, I realised very quickly that every change in data points can have an impact on correlation or regression. What caught my attention initially, was modifying the multiplier for the first data set, and as I was increasing the number, the correlation was flattening more and more. See attached two graphs for example:

<div align="center">

  <img src="relative/path/to/image.png" alt="Alternate text" width="100" />
  <br>
  <p>![Alt text](ml_source/c1.png)</p>

</div>

Figure 1: Initial Graph.
![Alt text](ml_source/c2.png)
Figure 2: Graph where first variable had increased multiplier to 100.
![Alt text](ml_source/c3.png)
Figure 3: Graph where first variable had increased multiplier to 1000.

Further playing with data only brought more fun and interesting learnings. It has also prompted me to research a bit more on the topic.

As data is integral to statistical analysis and both correlation and regression are statistical ways of measurement - it only comes natural, that any change in them can have a profound impact on any analysis.

What I found to be detrimental to correlation is outliers. Because they deviate from the rest of the data significantly, they can also significantly impact the output of a correlation calculation (Ravichandran, 2009). Outlier, as the name suggests, is something that is out of order and does not fall into the usual behaviour of the data points. Which is why there are statistical best practices on how to identify and then exclude such points in order to run the correlation exercise correctly (Abhigyan, 2020).

When it comes to regression, I believe there is no need to repeat myself, as the effects are very similar to those on correlation. Outliers can seriously affect the regression line which in turn leads to incorrect interpretations (James et al., 2013). 

I have always believed that Data Science is a science of a “best-guess”. It is hard to imagine being 100% confident in any statistical analysis that one runs. It is because of that, that we must adhere to the best practices and all of the tools that statistics provides us with, so we can make sure that our best-guess is indeed the best-guess one can make. 

References:

1. Abhigyan (2020) Detecting and removing outliers., Medium. Available at: https://medium.com/analytics-vidhya/detecting-and-removing-outliers-7b408b279c9 (Accessed: 13 July 2023). 

2. Ravichandran, K. S. (2009). A First Course in Linear Model Theory. CRC Press.

3.James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning: with Applications in R. Springer.


### Unit 4
### Unit 5
### Unit 6
### Unit 7
### Unit 8
### Unit 9
### Unit 10
### Unit 11
### Unit 12
